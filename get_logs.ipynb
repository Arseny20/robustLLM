{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMCzE+el4PYjJdMg4b9c0Cq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arseny20/robustLLM/blob/dataset/get_logs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import json\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\",\n",
        "                     model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\",\n",
        "                     device=0 if torch.cuda.is_available() else -1)"
      ],
      "metadata": {
        "id": "XpFr32HEymHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "df = load_dataset(\"ai4privacy/pii-masking-400k\")[\"train\"]\n",
        "df"
      ],
      "metadata": {
        "id": "0AxLnSFh0dnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edit_json(input: str):\n",
        "    json_dict = input\n",
        "    json_new = list()\n",
        "    for item in json_dict:\n",
        "        label_mapping = {\n",
        "            \"PHONE\": \"Phone\",\n",
        "            \"PASSWORD\": \"password\",\n",
        "            \"USERNAME\": \"login\",\n",
        "            \"EMAIL\": \"email\",\n",
        "            \"GIVENNAME\": \"FIO\",\n",
        "            \"SURNAME\": \"FIO\"\n",
        "        }\n",
        "        original_label = item[\"label\"]\n",
        "        item[\"label\"] = label_mapping.get(original_label, original_label)\n",
        "        if item[\"label\"] in [\"Phone\", \"password\", \"login\", \"email\", \"FIO\"]:\n",
        "            json_new.append(item)\n",
        "    return json_new\n",
        "\n",
        "\n",
        "def edit_text(text: str):\n",
        "    # Regular expression to match HTML tags\n",
        "    clean_text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Optional: Replace common HTML entities\n",
        "    html_entities = {\n",
        "        '&nbsp;': ' ',\n",
        "        '&amp;': '&',\n",
        "        '&lt;': '<',\n",
        "        '&gt;': '>',\n",
        "        '&quot;': '\"',\n",
        "        '&apos;': \"'\"\n",
        "    }\n",
        "\n",
        "    for entity, replacement in html_entities.items():\n",
        "        clean_text = clean_text.replace(entity, replacement)\n",
        "\n",
        "    return clean_text.strip()\n",
        "    return text\n",
        "\n",
        "def word_to_ner(text, label_data):\n",
        "    text = text.split()\n",
        "    idx = 0\n",
        "    new_list = list()\n",
        "    mapping = [\"Phone\", \"password\", \"login\", \"email\", \"FIO\"]\n",
        "    for word in text:\n",
        "        if idx < len(label_data) and word == label_data[idx][\"value\"]:\n",
        "            new_list.append(mapping.index(label_data[idx][\"label\"])+1)\n",
        "            idx += 1\n",
        "        else:\n",
        "            new_list.append(0)\n",
        "    return new_list\n",
        "\n",
        "\n",
        "def tokens_marking(tokens, token_classes):\n",
        "    token_mapping = {\n",
        "        \"B-PHONE\": \"B-Phone\",\n",
        "        \"B-PASSWORD\": \"B-password\",\n",
        "        \"B-USERNAME\": \"B-login\",\n",
        "        \"B-EMAIL\": \"B-email\",\n",
        "        \"B-GIVENNAME\": \"B-FIO\",\n",
        "        \"B-SURNAME\": \"B-FIO\",\n",
        "        \"I-PHONE\": \"I-Phone\",\n",
        "        \"I-PASSWORD\": \"I-password\",\n",
        "        \"I-USERNAME\": \"I-login\",\n",
        "        \"I-EMAIL\": \"I-email\",\n",
        "        \"I-GIVENNAME\": \"I-FIO\",\n",
        "        \"I-SURNAME\": \"I-FIO\"\n",
        "    }\n",
        "\n",
        "    # Convert map object to list to use it multiple times\n",
        "    token_classes = list(map(lambda x: token_mapping.get(x, 'O'), token_classes))\n",
        "\n",
        "    result = []\n",
        "    for token, cls in zip(tokens, token_classes):\n",
        "        result.append((token, cls))  # Fixed: Using tuple() requires a single iterable argument\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "oZWsdcFhLdcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\n",
        "    \"development_logs\",\n",
        "    \"application_logs\",\n",
        "    \"corporate_logs\",\n",
        "    \"other_logs\",\n",
        "    \"message\",\n",
        "    \"not_logs\",\n",
        "]\n",
        "\n",
        "\n",
        "texts = list()\n",
        "masks = list()\n",
        "indices = list()\n",
        "tokens = list()\n",
        "\n",
        "for i in range(3000): # здесь вставить кол-во записей\n",
        "    result = classifier(df[i][\"source_text\"], labels, multi_label=False)\n",
        "    if result['labels'][0] in labels[0:3]:\n",
        "        print(f\"{i:6}/{df.shape[0]} {result['labels'][0]:15} {df[i]['source_text'][:100]}...\")\n",
        "        json_to_insert = edit_json(df[i]['privacy_mask'])\n",
        "        if json_to_insert:\n",
        "            print(json_to_insert)\n",
        "            text_to_insert = edit_text(df[i]['source_text'])\n",
        "            ner_indices = word_to_ner(text_to_insert, json_to_insert)\n",
        "            tokens_to_insert = tokens_marking(df[i]['mbert_tokens'], df[i]['mbert_token_classes'])\n",
        "            texts.append(text_to_insert)\n",
        "            masks.append(json_to_insert)\n",
        "            indices.append(ner_indices)\n",
        "            tokens.append(tokens_to_insert)"
      ],
      "metadata": {
        "id": "CwmYSGKMzbVz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = pd.DataFrame({\n",
        "    \"source_text\": texts,\n",
        "    \"privacy_mask\": masks,\n",
        "    \"ner_indices\": indices,\n",
        "    \"tokens\": tokens\n",
        "})\n",
        "new_data.to_csv('logs_data.csv', index=None)\n",
        "new_data.head(10)"
      ],
      "metadata": {
        "id": "8iDk-Z7N1lVC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}