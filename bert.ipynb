{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde25d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mrars\\.cache\\huggingface\\hub\\models--DeepPavlov--xlm-roberta-large-en-ru. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[32m      3\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mDeepPavlov/xlm-roberta-large-en-ru\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDeepPavlov/xlm-roberta-large-en-ru\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:311\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:4680\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4671\u001b[39m     gguf_file\n\u001b[32m   4672\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4673\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4674\u001b[39m ):\n\u001b[32m   4675\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4676\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4677\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4678\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4680\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4682\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4687\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4693\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4695\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4698\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4700\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4701\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:1166\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[39m\n\u001b[32m   1163\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1164\u001b[39m         \u001b[38;5;66;03m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[32m   1165\u001b[39m         filename = _add_variant(WEIGHTS_NAME, variant)\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m         resolved_archive_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcached_file_kwargs\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename == _add_variant(WEIGHTS_NAME, variant):\n\u001b[32m   1170\u001b[39m     \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[32m   1171\u001b[39m     resolved_archive_file = cached_file(\n\u001b[32m   1172\u001b[39m         pretrained_model_name_or_path,\n\u001b[32m   1173\u001b[39m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[32m   1174\u001b[39m         **cached_file_kwargs,\n\u001b[32m   1175\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:312\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    255\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    256\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    257\u001b[39m     **kwargs,\n\u001b[32m    258\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    259\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    261\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    310\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:470\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    469\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    485\u001b[39m         snapshot_download(\n\u001b[32m    486\u001b[39m             path_or_repo_id,\n\u001b[32m    487\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    496\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    497\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1008\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    989\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    990\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1005\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1006\u001b[39m     )\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1161\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1158\u001b[39m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[32m   1160\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1173\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1174\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1725\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1718\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1719\u001b[39m             logger.warning(\n\u001b[32m   1720\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo, but the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhf_xet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m package is not installed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1721\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFalling back to regular HTTP download. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1722\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1723\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1725\u001b[39m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1726\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1727\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1728\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1729\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1730\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1731\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1732\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1734\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1735\u001b[39m _chmod_and_move(incomplete_path, destination_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:494\u001b[39m, in \u001b[36mhttp_get\u001b[39m\u001b[34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[39m\n\u001b[32m    492\u001b[39m new_resume_size = resume_size\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[32m    496\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:1091\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1094\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:980\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    977\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    978\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:904\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    901\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    906\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    912\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:887\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    884\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    885\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    886\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:708\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    710\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrars\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1106\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/xlm-roberta-large-en-ru\")\n",
    "model = AutoModel.from_pretrained(\"DeepPavlov/xlm-roberta-large-en-ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c1cb4935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "401115ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a9c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a4667558",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d4c38c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_log = ['80.97.168.58 - gorshkovnikon [Тимофеев Серафим Антонович] (email: viktor15@example.net, pass: _7Ys%IVQp2) phone=8 826 587 31 88 addr=\"к. Армавир, наб. Грибоедова, д. 9/3 стр. 5/3, 937143\" org=Медведева и партнеры ipv6=64be:ec33:33f1:1207:5d95:7f1:8710:a5','A support ticket was created by user jeffrey97 from organization Hill-Hall. The ticket relates to an issue reported by (575)679-5357x44829. The affected client has IP address 9468:d568:5c7e:3ece:55c2:8542:2c96:8ca1 and client ID 79dbb385-04e2-43ca-a6ec-dbd0ca04b741.']\n",
    "ner_data = [[\n",
    "  {\n",
    "    \"label\": \"ip\",\n",
    "    \"start\": 1,\n",
    "    \"end\": 13,\n",
    "    \"value\": \"80.97.168.58\"\n",
    "  },\n",
    "  {\n",
    "    \"label\": \"login\",\n",
    "    \"start\": 16,\n",
    "    \"end\": 29,\n",
    "    \"value\": \"gorshkovnikon\"\n",
    "  },\n",
    "  {\n",
    "    \"label\": \"fio\",\n",
    "    \"start\": 31,\n",
    "    \"end\": 57,\n",
    "    \"value\": \"Тимофеев Серафим Антонович\"\n",
    "  },\n",
    "  {\n",
    "    \"label\": \"email\",\n",
    "    \"start\": 67,\n",
    "    \"end\": 87,\n",
    "    \"value\": \"viktor15@example.net\"\n",
    "  },\n",
    "  {\n",
    "    \"label\": \"password\",\n",
    "    \"start\": 95,\n",
    "    \"end\": 105,\n",
    "    \"value\": \"_7Ys%IVQp2\"\n",
    "  },\n",
    "  {\n",
    "    \"label\": \"phone\",\n",
    "    \"start\": 113,\n",
    "    \"end\": 128,\n",
    "    \"value\": \"8 826 587 31 88\"\n",
    "  },\n",
    "  {\n",
    "    \"label\": \"address\",\n",
    "    \"start\": 135,\n",
    "    \"end\": 187,\n",
    "    \"value\": \"к. Армавир, наб. Грибоедова, д. 9/3 стр. 5/3, 937143\"\n",
    "  },\n",
    "  {\n",
    "    \"label\": \"org\",\n",
    "    \"start\": 193,\n",
    "    \"end\": 213,\n",
    "  },\n",
    "  {\n",
    "    \"label\": \"ipv6\",\n",
    "    \"start\": 219,\n",
    "    \"end\": 256,\n",
    "    \"value\": \"64be:ec33:33f1:1207:5d95:7f1:8710:a5a\"\n",
    "  }\n",
    "],[{'label': 'login', 'start': 38, 'end': 47, 'value': 'jeffrey97'},\n",
    " {'label': 'org', 'start': 66, 'end': 75, 'value': 'Hill-Hall'},\n",
    " {'label': 'phone', 'start': 120, 'end': 139, 'value': '(575)679-5357x44829'},\n",
    " {'label': 'ipv6',\n",
    "  'start': 176,\n",
    "  'end': 215,\n",
    "  'value': '9468:d568:5c7e:3ece:55c2:8542:2c96:8ca1'}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b901a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = ['address', 'email', 'fio', 'ip', 'ipv6', 'login', 'org', 'password', 'phone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ee73e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/xlm-roberta-large-en-ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "5f2e8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepared_data(data_log,ner_data,unique_labels):\n",
    "    df = pd.DataFrame({\n",
    "    \"sentence\": data_log,\n",
    "    \"ner\": ner_data,\n",
    "\n",
    "  })\n",
    "  #Creat label dictionary\n",
    "    unique_labels_ner = [\"O\"] + sorted([f\"B-{e}\" for e in unique_labels] + [f\"I-{e}\" for e in unique_labels])\n",
    "    label2id = {label: idx for idx, label in enumerate(unique_labels_ner)}\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "  \n",
    "  # Tokenize text\n",
    "    tokenized = df[\"sentence\"].apply(lambda x: tokenizer.encode_plus(\n",
    "        x,\n",
    "        add_special_tokens=True,  # Adds [CLS] and [SEP]\n",
    "        max_length=256,           # Pad/truncate to max length\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'       # Return PyTorch tensors\n",
    "    ))\n",
    "    df[\"input_ids\"] = tokenized.apply(lambda x: x[\"input_ids\"].squeeze(0))\n",
    "    df[\"attention_mask\"] = tokenized.apply(lambda x: x[\"attention_mask\"].squeeze(0))\n",
    "    df[\"offset_mapping\"] = tokenized.apply(lambda x: x[\"offset_mapping\"].squeeze(0))\n",
    "    return df,label2id,id2label,unique_labels_ner\n",
    "  #\n",
    "df,label2id,id2label,unique_labels_ner = prepared_data(data_log,ner_data,unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b36c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(ners,offsets,label2id):\n",
    "# Assign labels to tokens\n",
    "    total_labels = []\n",
    "    for ner,offset in zip(ners,offsets):\n",
    "        labels = [label2id[\"O\"]] * len(offset)\n",
    "        for entity in ner:\n",
    "            ent_start, ent_end = entity[\"start\"] - 1, entity[\"end\"] -1\n",
    "            ent_label = entity[\"label\"]\n",
    "            for idx,(start, end) in enumerate(offset):\n",
    "                custom_start = start\n",
    "                custom_end = end\n",
    "                if (start,end) == (0,0):\n",
    "                    labels[idx] = -100\n",
    "                elif((custom_start + 1>= ent_start) and (custom_end <= ent_end)):\n",
    "                    if (custom_start + 1)== ent_start:\n",
    "                        labels[idx] = label2id[f\"B-{ent_label}\"]\n",
    "                    else:\n",
    "                        labels[idx] = label2id[f\"I-{ent_label}\"]\n",
    "\n",
    "        total_labels.append(labels)\n",
    "    return total_labels\n",
    "df['labels'] = create_labels(df[\"ner\"],df[\"offset_mapping\"],label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "6f750e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_label(labels,idx,label2id,label_ner,start_ner =-1,start_token = 1):\n",
    "  if (start_token < start_ner):\n",
    "    labels[idx] = label2id[f\"B-{label_ner}\"]\n",
    "  else:\n",
    "    labels[idx] = label2id[f\"I-{label_ner}\"]\n",
    "  return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "b9015ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>ner</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>offset_mapping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80.97.168.58 - gorshkovnikon [Тимофеев Серафим...</td>\n",
       "      <td>[{'label': 'ip', 'start': 1, 'end': 13, 'value...</td>\n",
       "      <td>[tensor(0), tensor(1608), tensor(5), tensor(57...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[[tensor(0), tensor(0)], [tensor(0), tensor(2)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A support ticket was created by user jeffrey97...</td>\n",
       "      <td>[{'label': 'login', 'start': 38, 'end': 47, 'v...</td>\n",
       "      <td>[tensor(0), tensor(62), tensor(3798), tensor(2...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[[tensor(0), tensor(0)], [tensor(0), tensor(1)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  80.97.168.58 - gorshkovnikon [Тимофеев Серафим...   \n",
       "1  A support ticket was created by user jeffrey97...   \n",
       "\n",
       "                                                 ner  \\\n",
       "0  [{'label': 'ip', 'start': 1, 'end': 13, 'value...   \n",
       "1  [{'label': 'login', 'start': 38, 'end': 47, 'v...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [tensor(0), tensor(1608), tensor(5), tensor(57...   \n",
       "1  [tensor(0), tensor(62), tensor(3798), tensor(2...   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "\n",
       "                                      offset_mapping  \n",
       "0  [[tensor(0), tensor(0)], [tensor(0), tensor(2)...  \n",
       "1  [[tensor(0), tensor(0)], [tensor(0), tensor(1)...  "
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "10deb764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(ners,offsets,label2id):\n",
    "  total_labels = []\n",
    "  for ner,offset in zip(ners,offsets):\n",
    "    labels = [label2id[\"O\"]] * len(offset)\n",
    "    idx = 0\n",
    "    for item in offset:\n",
    "      start_token = item[0]\n",
    "      end_token = item[1]\n",
    "      if start_token == 0 and end_token == 0:\n",
    "        labels[idx] = -100 \n",
    "      else:\n",
    "        for entity in ner:\n",
    "          start_ner = entity[\"start\"]\n",
    "          end_ner = entity[\"end\"]\n",
    "          label_ner = entity[\"label\"]\n",
    "          if end_token >= start_ner and end_token < end_ner:\n",
    "            labels = mark_label(labels,idx,label2id,label_ner,start_ner,start_token)\n",
    "            break\n",
    "          elif end_ner > (start_token +1) and start_token > start_ner:\n",
    "            labels = mark_label(labels,idx,label2id,label_ner)\n",
    "            break\n",
    "      idx +=1\n",
    "    total_labels.append(labels)\n",
    "  return total_labels\n",
    "df['labels'] = create_labels(df[\"ner\"],df[\"offset_mapping\"],label2id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "4b4ee043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A support ticket was created by user jeffrey97 from organization Hill-Hall. The ticket relates to an issue reported by (575)679-5357x44829. The affected client has IP address 9468:d568:5c7e:3ece:55c2:8542:2c96:8ca1 and client ID 79dbb385-04e2-43ca-a6ec-dbd0ca04b741.'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "3131508d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'login', 'start': 38, 'end': 47, 'value': 'jeffrey97'},\n",
       " {'label': 'org', 'start': 66, 'end': 75, 'value': 'Hill-Hall'},\n",
       " {'label': 'phone', 'start': 120, 'end': 139, 'value': '(575)679-5357x44829'},\n",
       " {'label': 'ipv6',\n",
       "  'start': 176,\n",
       "  'end': 215,\n",
       "  'value': '9468:d568:5c7e:3ece:55c2:8542:2c96:8ca1'}]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ner'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "a3e63408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>             → IGNORED, tensor([0, 0])\n",
      "▁A              → O, tensor([0, 1])\n",
      "▁support        → O, tensor([1, 9])\n",
      "▁ticket         → O, tensor([ 9, 16])\n",
      "▁was            → O, tensor([16, 20])\n",
      "▁created        → O, tensor([20, 28])\n",
      "▁by             → O, tensor([28, 31])\n",
      "▁user           → O, tensor([31, 36])\n",
      "▁je             → B-login, tensor([36, 39])\n",
      "ff              → I-login, tensor([39, 41])\n",
      "rey             → I-login, tensor([41, 44])\n",
      "97              → I-login, tensor([44, 46])\n",
      "▁from           → O, tensor([46, 51])\n",
      "▁organization   → O, tensor([51, 64])\n",
      "▁Hill           → B-org, tensor([64, 69])\n",
      "-               → I-org, tensor([69, 70])\n",
      "H               → I-org, tensor([70, 71])\n",
      "all             → I-org, tensor([71, 74])\n",
      ".               → O, tensor([74, 75])\n",
      "▁The            → O, tensor([75, 79])\n",
      "▁ticket         → O, tensor([79, 86])\n",
      "▁relat          → O, tensor([86, 92])\n",
      "es              → O, tensor([92, 94])\n",
      "▁to             → O, tensor([94, 97])\n",
      "▁an             → O, tensor([ 97, 100])\n",
      "▁issue          → O, tensor([100, 106])\n",
      "▁reported       → O, tensor([106, 115])\n",
      "▁by             → O, tensor([115, 118])\n",
      "▁(5             → B-phone, tensor([118, 121])\n",
      "75              → I-phone, tensor([121, 123])\n",
      ")               → I-phone, tensor([123, 124])\n",
      "6               → I-phone, tensor([124, 125])\n",
      "79              → I-phone, tensor([125, 127])\n",
      "-               → I-phone, tensor([127, 128])\n",
      "53              → I-phone, tensor([128, 130])\n",
      "57              → I-phone, tensor([130, 132])\n",
      "x               → I-phone, tensor([132, 133])\n",
      "4               → I-phone, tensor([133, 134])\n",
      "48              → I-phone, tensor([134, 136])\n",
      "29              → I-phone, tensor([136, 138])\n",
      ".               → O, tensor([138, 139])\n",
      "▁The            → O, tensor([139, 143])\n",
      "▁affected       → O, tensor([143, 152])\n",
      "▁client         → O, tensor([152, 159])\n",
      "▁has            → O, tensor([159, 163])\n",
      "▁IP             → O, tensor([163, 166])\n",
      "▁address        → O, tensor([166, 174])\n",
      "▁94             → B-ipv6, tensor([174, 177])\n",
      "68              → I-ipv6, tensor([177, 179])\n",
      ":               → I-ipv6, tensor([179, 180])\n",
      "d               → I-ipv6, tensor([180, 181])\n",
      "5               → I-ipv6, tensor([181, 182])\n",
      "68              → I-ipv6, tensor([182, 184])\n",
      ":               → I-ipv6, tensor([184, 185])\n",
      "5               → I-ipv6, tensor([185, 186])\n",
      "c               → I-ipv6, tensor([186, 187])\n",
      "7               → I-ipv6, tensor([187, 188])\n",
      "e               → I-ipv6, tensor([188, 189])\n",
      ":               → I-ipv6, tensor([189, 190])\n",
      "3               → I-ipv6, tensor([190, 191])\n",
      "ece             → I-ipv6, tensor([191, 194])\n",
      ":55             → I-ipv6, tensor([194, 197])\n",
      "c               → I-ipv6, tensor([197, 198])\n",
      "2               → I-ipv6, tensor([198, 199])\n",
      ":               → I-ipv6, tensor([199, 200])\n",
      "85              → I-ipv6, tensor([200, 202])\n",
      "42              → I-ipv6, tensor([202, 204])\n",
      ":               → I-ipv6, tensor([204, 205])\n",
      "2               → I-ipv6, tensor([205, 206])\n",
      "c               → I-ipv6, tensor([206, 207])\n",
      "96              → I-ipv6, tensor([207, 209])\n",
      ":               → I-ipv6, tensor([209, 210])\n",
      "8               → I-ipv6, tensor([210, 211])\n",
      "ca              → I-ipv6, tensor([211, 213])\n",
      "1               → I-ipv6, tensor([213, 214])\n",
      "▁and            → O, tensor([214, 218])\n",
      "▁client         → O, tensor([218, 225])\n",
      "▁ID             → O, tensor([225, 228])\n",
      "▁79             → O, tensor([228, 231])\n",
      "d               → O, tensor([231, 232])\n",
      "bb              → O, tensor([232, 234])\n",
      "385             → O, tensor([234, 237])\n",
      "-               → O, tensor([237, 238])\n",
      "04              → O, tensor([238, 240])\n",
      "e               → O, tensor([240, 241])\n",
      "2-              → O, tensor([241, 243])\n",
      "43              → O, tensor([243, 245])\n",
      "ca              → O, tensor([245, 247])\n",
      "-               → O, tensor([247, 248])\n",
      "a               → O, tensor([248, 249])\n",
      "6               → O, tensor([249, 250])\n",
      "ec              → O, tensor([250, 252])\n",
      "-               → O, tensor([252, 253])\n",
      "db              → O, tensor([253, 255])\n",
      "d               → O, tensor([255, 256])\n",
      "0               → O, tensor([256, 257])\n",
      "ca              → O, tensor([257, 259])\n",
      "04              → O, tensor([259, 261])\n",
      "b               → O, tensor([261, 262])\n",
      "74              → O, tensor([262, 264])\n",
      "1               → O, tensor([264, 265])\n",
      ".               → O, tensor([265, 266])\n",
      "</s>            → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n",
      "<pad>           → IGNORED, tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "def test_tokens(labels,input_ids,offset):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    token_labels =  [id2label[l] if l != -100 else \"IGNORED\" for l in labels]\n",
    "    for t, l, o in zip(tokens, token_labels,offset):\n",
    "        print(f\"{t:15} → {l}, {o}\")\n",
    "test_tokens(df['labels'][1],df['input_ids'][1],df[\"offset_mapping\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "53ab8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_log2 = ['A support ticket was created by user jeffrey97 from organization Hill-Hall. The ticket relates to an issue reported by (575)679-5357x44829. The affected client has IP address 9468:d568:5c7e:3ece:55c2:8542:2c96:8ca1 and client ID 79dbb385-04e2-43ca-a6ec-dbd0ca04b741.']\n",
    "ner_data2 = [{'label': 'login', 'start': 38, 'end': 47, 'value': 'jeffrey97'},\n",
    " {'label': 'org', 'start': 66, 'end': 75, 'value': 'Hill-Hall'},\n",
    " {'label': 'phone', 'start': 120, 'end': 139, 'value': '(575)679-5357x44829'},\n",
    " {'label': 'ipv6',\n",
    "  'start': 176,\n",
    "  'end': 215,\n",
    "  'value': '9468:d568:5c7e:3ece:55c2:8542:2c96:8ca1'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ce203fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = ['address', 'email', 'fio', 'ip', 'ipv6', 'login', 'org', 'password', 'phone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1eabdcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepared_data(data_log,ner_data,unique_labels):\n",
    "  df = pd.DataFrame({\n",
    "    \"sentence\": data_log,\n",
    "    \"ner\": ner_data,\n",
    "\n",
    "  })\n",
    "  #Creat label dictionary\n",
    "  unique_labels_ner = [\"O\"] + sorted([f\"B-{e}\" for e in unique_labels] + [f\"I-{e}\" for e in unique_labels])\n",
    "  label2id = {label: idx for idx, label in enumerate(unique_labels_ner)}\n",
    "  id2label = {v: k for k, v in label2id.items()}\n",
    "  \n",
    "  # Tokenize text\n",
    "  tokenized = df[\"sentence\"].apply(lambda x: tokenizer.encode_plus(\n",
    "        x,\n",
    "        add_special_tokens=True,  # Adds [CLS] and [SEP]\n",
    "        max_length=128,           # Pad/truncate to max length\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'       # Return PyTorch tensors\n",
    "    ))\n",
    "  df[\"input_ids\"] = tokenized.apply(lambda x: x[\"input_ids\"].squeeze(0))\n",
    "  df[\"attention_mask\"] = tokenized.apply(lambda x: x[\"attention_mask\"].squeeze(0))\n",
    "  df[\"offset_mapping\"] = tokenized.apply(lambda x: x[\"offset_mapping\"].squeeze(0))\n",
    "  return df,label2id,id2label,unique_labels_ner\n",
    "  #\n",
    "df,label2id,id2label,unique_labels_ner = prepared_data(data_log,ner_data,unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ad256fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-Address',\n",
       " 'B-FIO',\n",
       " 'B-Organization',\n",
       " 'B-Phone',\n",
       " 'B-email',\n",
       " 'B-ipv4',\n",
       " 'B-ipv6',\n",
       " 'B-login',\n",
       " 'B-password',\n",
       " 'I-Address',\n",
       " 'I-FIO',\n",
       " 'I-Organization',\n",
       " 'I-Phone',\n",
       " 'I-email',\n",
       " 'I-ipv4',\n",
       " 'I-ipv6',\n",
       " 'I-login',\n",
       " 'I-password']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "86349bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(ners,offsets,label2id):\n",
    "# Assign labels to tokens\n",
    "  total_labels = []\n",
    "  for ner,offset in zip(ners,offsets):\n",
    "    labels = [label2id[\"O\"]] * len(offset)\n",
    "    for entity in ner:\n",
    "        ent_start, ent_end = entity[\"start\"], entity[\"end\"]\n",
    "        ent_label = entity[\"label\"]\n",
    "        for idx, (start, end) in enumerate(offset[1:-1]):\n",
    "          idx += 1\n",
    "          start += 1\n",
    "          if (start >= ent_start and end <= ent_end):\n",
    "              if (start)== ent_start:\n",
    "                  labels[idx] = label2id[f\"B-{ent_label}\"]\n",
    "              else:\n",
    "                  labels[idx] = label2id[f\"I-{ent_label}\"]\n",
    "    total_labels.append(labels)\n",
    "  return total_labels\n",
    "df['labels'] = create_labels(df[\"ner\"],df[\"offset_mapping\"],label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09201d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "822aacbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Token-label pairs saved as Python list in ner_output.txt\n"
     ]
    }
   ],
   "source": [
    "def test_tokens_to_list_file(labels, input_ids, tokenizer, id2label, output_file=\"ner_list.txt\"):\n",
    "    \"\"\"\n",
    "    Writes tokens and their labels as a Python list of lists to a text file.\n",
    "\n",
    "    Args:\n",
    "        labels (list of list): List of label IDs for each sentence.\n",
    "        input_ids (list of list): List of token IDs for each sentence.\n",
    "        tokenizer: Hugging Face tokenizer.\n",
    "        id2label (dict): Mapping from label IDs to label names.\n",
    "        output_file (str): Path to output file.\n",
    "    \"\"\"\n",
    "    all_sentences = []\n",
    "\n",
    "    for label_seq, input_id_seq in zip(labels, input_ids):\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_id_seq)\n",
    "        token_labels = [id2label[l] for l in label_seq]\n",
    "\n",
    "        # Filter out special tokens\n",
    "        filtered = [\n",
    "            (token, tag)\n",
    "            for token, tag in zip(tokens, token_labels)\n",
    "            if token not in tokenizer.all_special_tokens\n",
    "        ]\n",
    "        all_sentences.append(filtered)\n",
    "\n",
    "    # Write to file as Python list\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"[\\n\")\n",
    "        for sentence in all_sentences:\n",
    "            f.write(\"  [\\n\")\n",
    "            for token, tag in sentence:\n",
    "                f.write(f\"    ({repr(token)}, {repr(tag)}),\\n\")\n",
    "            f.write(\"  ],\\n\")\n",
    "        f.write(\"]\\n\")\n",
    "\n",
    "    print(f\"✅ Token-label pairs saved as Python list in {output_file}\")\n",
    "\n",
    "test_tokens_to_list_file(\n",
    "    df['labels'],\n",
    "    df['input_ids'],\n",
    "    tokenizer,\n",
    "    id2label,\n",
    "    output_file=\"ner_output.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "25fd4852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           → O\n",
      "80              → B-ipv4\n",
      ".               → I-ipv4\n",
      "97              → I-ipv4\n",
      ".               → I-ipv4\n",
      "168             → I-ipv4\n",
      ".               → I-ipv4\n",
      "58              → I-ipv4\n",
      "-               → O\n",
      "go              → I-login\n",
      "##rs            → I-login\n",
      "##h             → I-login\n",
      "##kov           → I-login\n",
      "##nik           → I-login\n",
      "##on            → I-login\n",
      "[               → I-FIO\n",
      "Тим             → I-FIO\n",
      "##о             → I-FIO\n",
      "##фе            → I-FIO\n",
      "##ев            → I-FIO\n",
      "Се              → I-FIO\n",
      "##раф           → I-FIO\n",
      "##им            → I-FIO\n",
      "Антон           → I-FIO\n",
      "##ович          → I-FIO\n",
      "]               → I-FIO\n",
      "(               → O\n",
      "email           → O\n",
      ":               → I-email\n",
      "vik             → I-email\n",
      "##tor           → I-email\n",
      "##15            → I-email\n",
      "@               → I-email\n",
      "example         → I-email\n",
      ".               → I-email\n",
      "net             → I-email\n",
      ",               → I-email\n",
      "pass            → O\n",
      ":               → I-password\n",
      "_               → I-password\n",
      "7               → I-password\n",
      "##Y             → I-password\n",
      "##s             → I-password\n",
      "%               → I-password\n",
      "IV              → I-password\n",
      "##Q             → I-password\n",
      "##p             → I-password\n",
      "##2             → I-password\n",
      ")               → I-password\n",
      "phone           → O\n",
      "=               → I-Phone\n",
      "8               → I-Phone\n",
      "826             → I-Phone\n",
      "587             → I-Phone\n",
      "31              → I-Phone\n",
      "88              → I-Phone\n",
      "add             → B-Address\n",
      "##r             → I-Address\n",
      "=               → I-Address\n",
      "\"               → I-Address\n",
      "к               → I-Address\n",
      ".               → I-Address\n",
      "А               → I-Address\n",
      "##рма           → I-Address\n",
      "##ви            → I-Address\n",
      "##р             → I-Address\n",
      ",               → I-Address\n",
      "на              → I-Address\n",
      "##б             → I-Address\n",
      ".               → I-Address\n",
      "Г               → I-Address\n",
      "##ри            → I-Address\n",
      "##бо            → I-Address\n",
      "##ед            → I-Address\n",
      "##ова           → I-Address\n",
      ",               → I-Address\n",
      "д               → I-Address\n",
      ".               → I-Address\n",
      "9               → I-Address\n",
      "/               → I-Address\n",
      "3               → I-Address\n",
      "стр             → I-Address\n",
      ".               → I-Address\n",
      "5               → I-Address\n",
      "/               → I-Address\n",
      "3               → I-Address\n",
      ",               → I-Address\n",
      "937             → I-Address\n",
      "##14            → I-Address\n",
      "##3             → B-Organization\n",
      "\"               → I-Organization\n",
      "org             → I-Organization\n",
      "=               → I-Organization\n",
      "М               → I-Organization\n",
      "##ед            → I-Organization\n",
      "##веде          → I-Organization\n",
      "##ва            → I-Organization\n",
      "и               → I-Organization\n",
      "пар             → I-Organization\n",
      "##тне           → I-Organization\n",
      "##ры            → B-ipv6\n",
      "i               → I-ipv6\n",
      "##p             → I-ipv6\n",
      "##v             → I-ipv6\n",
      "##6             → I-ipv6\n",
      "=               → I-ipv6\n",
      "64              → I-ipv6\n",
      "##be            → I-ipv6\n",
      ":               → I-ipv6\n",
      "e               → I-ipv6\n",
      "##c             → I-ipv6\n",
      "##33            → I-ipv6\n",
      ":               → I-ipv6\n",
      "33              → I-ipv6\n",
      "##f             → I-ipv6\n",
      "##1             → I-ipv6\n",
      ":               → I-ipv6\n",
      "1207            → I-ipv6\n",
      ":               → I-ipv6\n",
      "5               → I-ipv6\n",
      "##d             → I-ipv6\n",
      "##9             → I-ipv6\n",
      "##5             → I-ipv6\n",
      ":               → I-ipv6\n",
      "7               → I-ipv6\n",
      "##f             → I-ipv6\n",
      "##1             → I-ipv6\n",
      "[SEP]           → O\n",
      "[CLS]           → O\n",
      "Barack          → O\n",
      "Obama           → O\n",
      "was             → O\n",
      "the             → O\n",
      "44              → O\n",
      "##th            → O\n",
      "US              → O\n",
      "president       → O\n",
      ".               → O\n",
      "[SEP]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n",
      "[PAD]           → O\n"
     ]
    }
   ],
   "source": [
    "def test_tokens(labels,input_ids):\n",
    "  for label,input_id in zip(labels,input_ids):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_id)\n",
    "    token_labels = [id2label[l] for l in label]\n",
    "    for t, l in zip(tokens, token_labels):\n",
    "        print(f\"{t:15} → {l}\")\n",
    "test_tokens(df['labels'],df['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "161a6781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenClassifierHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        \"\"\"\n",
    "        Classification head for token-level predictions.\n",
    "        \"\"\"\n",
    "        super(TokenClassifierHead, self).__init__()\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor of shape [batch_size, seq_len, hidden_size]\n",
    "        Returns:\n",
    "            logits: tensor of shape [batch_size, seq_len, num_labels]\n",
    "        \"\"\"\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c3a9c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForNER(nn.Module):\n",
    "    def __init__(self, model, num_labels):\n",
    "        \"\"\"\n",
    "        BERT + token classification head for NER.\n",
    "        \"\"\"\n",
    "        super(BERTForNER, self).__init__()\n",
    "        self.bert = model\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.classifier = TokenClassifierHead(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            attention_mask: [batch_size, seq_len]\n",
    "            token_type_ids: [batch_size, seq_len]\n",
    "        Returns:\n",
    "            logits: [batch_size, seq_len, num_labels]\n",
    "        \"\"\"\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        # Get all token embeddings\n",
    "        sequence_output = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        # Predict logits for each token\n",
    "        logits = self.classifier(sequence_output)  # [batch_size, seq_len, num_labels]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9966e18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bc81944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, input_ids,attention_mask, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encodings: tokenizer output (input_ids, attention_mask, etc.)\n",
    "            labels: list of BIO tag IDs aligned with tokens\n",
    "        \"\"\"\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item ={}\n",
    "        item['input_ids'] =  torch.tensor(self.input_ids[idx])\n",
    "        item['attention_mask']  = torch.tensor(self.attention_mask[idx])\n",
    "        item[\"labels\"] =  torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "485ba87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,df,unique_labels_ner,epoch):\n",
    "    num_labels = len(unique_labels_ner)\n",
    "    input_ids = df['input_ids']\n",
    "    attention_mask = df['attention_mask']\n",
    "    labels = df['labels']\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_test = BERTForNER(model, num_labels).to(device)\n",
    "    optimizer = AdamW(model_test.parameters(), lr=5e-5)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    dataset = NERDataset(input_ids,attention_mask, labels)  # Assume you preprocessed your data\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    epochs = epoch\n",
    "    for epoch in range(epochs):\n",
    "        model_test.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            print(type(batch[\"input_ids\"]))\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            if len(input_ids.shape) == 1:\n",
    "                input_ids = input_ids.unsqueeze(0)\n",
    "                attention_mask = attention_mask.unsqueeze(0)\n",
    "            # Forward\n",
    "            logits = model_test(input_ids, attention_mask)\n",
    "\n",
    "            # Reshape logits and labels for loss\n",
    "            loss = criterion(\n",
    "                logits.view(-1, num_labels),  # [batch * seq_len, num_labels]\n",
    "                labels.view(-1)               # [batch * seq_len]\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "    return model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f4b8ce07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrars\\AppData\\Local\\Temp\\ipykernel_32884\\2707286675.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['input_ids'] =  torch.tensor(self.input_ids[idx])\n",
      "C:\\Users\\mrars\\AppData\\Local\\Temp\\ipykernel_32884\\2707286675.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['attention_mask']  = torch.tensor(self.attention_mask[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Epoch 1/5 - Loss: 3.2802\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 2/5 - Loss: 1.9860\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 3/5 - Loss: 1.2943\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 4/5 - Loss: 0.9800\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 5/5 - Loss: 0.8118\n"
     ]
    }
   ],
   "source": [
    "model_test = train(model,df,unique_labels_ner,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "26cb7732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "590f8454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, tokenizer, id2label, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Run NER prediction on raw text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw input text.\n",
    "        model (nn.Module): Your trained BERTForNER model.\n",
    "        tokenizer: Hugging Face tokenizer.\n",
    "        id2label (dict): Mapping from label IDs to label names.\n",
    "        device (str): \"cpu\" or \"cuda\".\n",
    "    Returns:\n",
    "        List of (token, predicted_label)\n",
    "    \"\"\"\n",
    "    # Tokenize input text\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,  # Adds [CLS] and [SEP]\n",
    "        max_length=128,           # Pad/truncate to max length\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'       # Return PyTorch tensors\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        predictions = torch.argmax(logits, dim=-1)  # [batch_size, seq_len]\n",
    "\n",
    "    # Convert IDs to tokens and labels\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    predicted_labels = [id2label[p.item()] for p in predictions[0]]\n",
    "\n",
    "    # Filter out special tokens ([CLS], [SEP], [PAD])\n",
    "    filtered_results = []\n",
    "    for token, label in zip(tokens, predicted_labels):\n",
    "        if token not in tokenizer.all_special_tokens:\n",
    "            filtered_results.append((token, label))\n",
    "\n",
    "    return filtered_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "595b06d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80              → I-FIO\n",
      ".               → I-FIO\n",
      "97              → I-ipv4\n",
      ".               → I-FIO\n",
      "168             → I-ipv4\n",
      ".               → I-FIO\n",
      "58              → I-ipv4\n",
      "-               → I-FIO\n",
      "go              → I-FIO\n",
      "##rs            → I-FIO\n",
      "##h             → I-FIO\n",
      "##kov           → I-FIO\n",
      "##nik           → I-FIO\n",
      "##on            → I-FIO\n",
      "[               → I-FIO\n",
      "Тим             → I-FIO\n",
      "##о             → I-FIO\n",
      "##фе            → I-FIO\n",
      "##ев            → I-FIO\n",
      "Се              → I-FIO\n",
      "##раф           → I-FIO\n",
      "##им            → I-FIO\n",
      "Антон           → I-FIO\n",
      "##ович          → I-FIO\n",
      "]               → I-FIO\n",
      "(               → O\n",
      "email           → O\n",
      ":               → I-password\n",
      "vik             → I-password\n",
      "##tor           → I-password\n",
      "##15            → I-password\n",
      "@               → I-email\n",
      "example         → I-password\n",
      ".               → I-email\n",
      "net             → I-password\n",
      ",               → I-password\n",
      "pass            → I-password\n",
      ":               → I-password\n",
      "_               → I-password\n",
      "7               → I-password\n",
      "##Y             → I-password\n",
      "##s             → I-password\n",
      "%               → I-password\n",
      "IV              → I-password\n",
      "##Q             → I-password\n",
      "##p             → I-password\n",
      "##2             → I-password\n",
      ")               → I-password\n",
      "phone           → O\n",
      "=               → O\n",
      "8               → I-password\n",
      "826             → I-password\n",
      "587             → O\n",
      "31              → I-password\n",
      "88              → I-ipv6\n",
      "add             → I-Address\n",
      "##r             → I-Address\n",
      "=               → I-Address\n",
      "\"               → I-Address\n",
      "к               → I-Address\n",
      ".               → I-Address\n",
      "А               → I-Address\n",
      "##рма           → I-Address\n",
      "##ви            → I-Address\n",
      "##р             → I-Address\n",
      ",               → I-Address\n",
      "на              → I-Address\n",
      "##б             → I-Address\n",
      ".               → I-Address\n",
      "Г               → I-Address\n",
      "##ри            → I-Address\n",
      "##бо            → I-Address\n",
      "##ед            → I-Address\n",
      "##ова           → I-Address\n",
      ",               → I-Address\n",
      "д               → I-Address\n",
      ".               → I-Address\n",
      "9               → I-Address\n",
      "/               → I-Address\n",
      "3               → I-Address\n",
      "стр             → I-Address\n",
      ".               → I-Address\n",
      "5               → I-Address\n",
      "/               → I-Address\n",
      "3               → I-Address\n",
      ",               → I-Address\n",
      "937             → I-Address\n",
      "##14            → I-Address\n",
      "##3             → I-Address\n",
      "\"               → I-Address\n",
      "org             → I-Address\n",
      "=               → I-Address\n",
      "М               → I-Address\n",
      "##ед            → I-Address\n",
      "##веде          → I-Address\n",
      "##ва            → I-ipv6\n",
      "и               → I-ipv6\n",
      "пар             → I-ipv6\n",
      "##тне           → I-ipv6\n",
      "##ры            → I-ipv6\n",
      "i               → I-ipv6\n",
      "##p             → I-ipv6\n",
      "##v             → I-ipv6\n",
      "##6             → I-ipv6\n",
      "=               → I-ipv6\n",
      "64              → I-ipv6\n",
      "##be            → I-ipv6\n",
      ":               → I-ipv6\n",
      "e               → I-ipv6\n",
      "##c             → I-ipv6\n",
      "##33            → I-ipv6\n",
      ":               → I-ipv6\n",
      "33              → I-ipv6\n",
      "##f             → I-ipv6\n",
      "##1             → I-ipv6\n",
      ":               → I-ipv6\n",
      "1207            → I-ipv6\n",
      ":               → I-ipv6\n",
      "5               → I-ipv6\n",
      "##d             → I-ipv6\n",
      "##9             → I-ipv6\n",
      "##5             → I-ipv6\n",
      ":               → I-ipv6\n",
      "7               → I-ipv6\n",
      "##f             → I-ipv6\n",
      "##1             → I-ipv6\n"
     ]
    }
   ],
   "source": [
    "# Assume your trained model + tokenizer are loaded\n",
    "text = data_log[0]\n",
    "\n",
    "results = predict(text, model_test, tokenizer, id2label, device=\"cuda\")\n",
    "for token, label in results:\n",
    "    print(f\"{token:15} → {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0408482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
